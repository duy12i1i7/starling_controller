#!/usr/bin/env bash
# Starts Starling Server

set -e
set -o errexit


cli_help_install() {
    echo -e "
Command: start [kind/k3s] (options?)

Usage:
    No Arguments\t\tStarts kubernetes and deployment only
    kind/k3s\t\tStarts kubernetes and starts the simulator deployment
    dashboard\t\tStarts kubernetes dashboard

Options
    --num_uavs -n\t Maximum number of uavs to start (1 by default)
    --num_ugvs -n_ugv\t Maximum number of ground vehicles to start (0 by default)
    --vehicle_config_yaml\t Yaml file dictating the vehicle setup. Accepting a list of 'vehicles', specifying 'type', 'x', 'y' and 'z'
"
}

# Check if running using starling cli
if [ -z ${STARLING_COMMON_SOURCED+'check'} ]; then
    echo "Common not sourced, please run from starling cli"
    exit 1
fi

NUM_VEH=1
# Parse arguments
while [[ $# -gt 0 ]]
do
    key="$1"
    case $key in
        -h|--help|help)
            cli_help_install
            exit 1
            ;;
        -n|--num_uavs)
            NUM_VEH=$2
            shift
            shift
            ;;
        -n_ugv|--num_ugvs)
            NUM_UGV=$2
            shift
            shift
            ;;
        --vehicle_config_yaml)
            VEHICLE_CONFIG_YAML=$2
            shift
            shift
            ;;
        kind)
            S_KIND=1
            shift
            ;;
        k3s)
            S_K3S=1
            shift
            ;;
        dashboard)
            S_DASH=1
            shift
            ;;
        *)  # unknown option
            echo "Unknown Argument/ Option $1"
            cli_help_install
            exit 1
            shift # past argument
            ;;
    esac
done

# Start k3s

# Start kind.
# This creates a simulated multi-node cluster locally.
# The kind config must therefore be dynamically built depending on the number of vehicles in the cluster
# For each node, a vehicle.config is dynamically created, mirroring the real vehicle setup
# Both configurations are put in a temporary /tmp folder
# These files are then mounted into the kind node containers
if [[ $S_KIND ]]; then

    controlplanenode="starling-cluster-control-plane"
    echo "Control plane node: $controlplanenode"

    if [[ ! $VEHICLE_CONFIG_YAML ]]; then
        ## Write Vehicle Configuration files into tmp folder after creating tmp folder
        ## These mirror the config files found on real vehicles

        ## Determine number of vehicles (default 1)
        echo "Number of UAVs set to $NUM_VEH"

        mkdir -p "${STARLING_TMP_DIR}"
        for (( idx=0; idx<NUM_VEH; idx++ )); do
            CONFPATH="${STARLING_TMP_DIR}/vehicle${idx}.config"
            touch "$CONFPATH"
            # PX4_INSTANCE is required otherwise vehicles all default to zero
            # PX4_SIM_HOST is for SITL connecting to gazebo instance on the control plane node
            # PX4_SIM_INIT_LOC_X is to ensure multiple vehicles do not spawn on top of each other
            # VEHILCE_FCU_URL is set as mavros only autogenerates if config is not provided. We provide config, so we must recreate the URL
            { echo "PX4_INSTANCE=${idx}";
            echo "PX4_SIM_HOST=${controlplanenode}";
            echo "PX4_SIM_INIT_LOC_X=$((idx))";
            echo "VEHICLE_FCU_URL=udp://127.0.0.1:$((14830+idx))@/";
            echo "VEHICLE_FIRMWARE=px4";
            echo "VEHICLE_MAVLINK_SYSID=$((idx+1))";
            echo "VEHICLE_NAME=vehicle_${idx}";} > "$CONFPATH"
            touch "${STARLING_TMP_DIR}/px4fmu_vehicle${idx}"
        done

        if [[ $NUM_UGV ]]; then
            echo "Number of UGVs set to $NUM_UGV"
            for (( idx=NUM_VEH; idx<NUM_VEH+NUM_UGV; idx++ )); do
                CONFPATH="${STARLING_TMP_DIR}/vehicle${idx}.config"
                touch "$CONFPATH"
                # PX4_INSTANCE is required otherwise vehicles all default to zero
                # PX4_SIM_HOST is for SITL connecting to gazebo instance on the control plane node
                # PX4_SIM_INIT_LOC_X is to ensure multiple vehicles do not spawn on top of each other
                # VEHILCE_FCU_URL is set as mavros only autogenerates if config is not provided. We provide config, so we must recreate the URL
                { echo "PX4_INSTANCE=${idx}";
                echo "PX4_SIM_HOST=${controlplanenode}";
                echo "PX4_SIM_MODEL=r1-rover" # Add this one for rover
                echo "PX4_SIM_INIT_LOC_X=$((idx))";
                echo "VEHICLE_FCU_URL=udp://127.0.0.1:$((14830+idx))@/";
                echo "VEHICLE_FIRMWARE=px4";
                echo "VEHICLE_MAVLINK_SYSID=$((idx+1))";
                echo "VEHICLE_NAME=vehicle_${idx}";} > "$CONFPATH"
                touch "${STARLING_TMP_DIR}/px4fmu_vehicle${idx}"
            done
        fi

        TOTAL_NUM_VEH=$((NUM_VEH+NUM_UGV))

    else
        echo "Vehicle config yaml specified at $VEHICLE_CONFIG_YAML"
        TOTAL_NUM_VEH=$(yq ".vehicles | length" "$VEHICLE_CONFIG_YAML")
        echo "Number of vehicles set to $TOTAL_NUM_VEH"

        mkdir -p "${STARLING_TMP_DIR}"
        for (( idx=0; idx<TOTAL_NUM_VEH; idx++ )); do
            CONFPATH="${STARLING_TMP_DIR}/vehicle${idx}.config"
            touch "$CONFPATH"
            # PX4_INSTANCE is required otherwise vehicles all default to zero
            # PX4_SIM_HOST is for SITL connecting to gazebo instance on the control plane node
            # PX4_SIM_INIT_LOC_X is to ensure multiple vehicles do not spawn on top of each other
            # VEHILCE_FCU_URL is set as mavros only autogenerates if config is not provided. We provide config, so we must recreate the URL
            { echo "PX4_INSTANCE=${idx}";
            echo "PX4_SIM_HOST=${controlplanenode}";
            echo "VEHICLE_FCU_URL=udp://127.0.0.1:$((14830+idx))@/";
            echo "VEHICLE_FIRMWARE=px4";
            echo "VEHICLE_MAVLINK_SYSID=$((idx+1))";
            echo "VEHICLE_NAME=vehicle_${idx}";} > "$CONFPATH"

            VEHICLE_TYPE=$(yq ".vehicles[${idx}].type" "${VEHICLE_CONFIG_YAML}")
            if [[ $VEHICLE_TYPE != "uav" ]]; then
                echo "PX4_SIM_MODEL=${VEHICLE_TYPE}" >> "$CONFPATH"
            fi

            X_LOC=$(yq ".vehicles[${idx}].x" "${VEHICLE_CONFIG_YAML}")
            if [[ "${X_LOC}" != "null" ]]; then
                echo "PX4_SIM_INIT_LOC_X=$X_LOC" >> "$CONFPATH"
            else
                echo "PX4_SIM_INIT_LOC_X=$((idx))" >> "$CONFPATH"
            fi

            Y_LOC=$(yq ".vehicles[${idx}].y" "${VEHICLE_CONFIG_YAML}")
            if [[ "${Y_LOC}" != "null" ]]; then
                echo "PX4_SIM_INIT_LOC_Y=$Y_LOC" >> "$CONFPATH"
            fi

            Z_LOC=$(yq ".vehicles[${idx}].z" "${VEHICLE_CONFIG_YAML}")
            if [[ "${Z_LOC}" != "null" ]]; then
                echo "PX4_SIM_INIT_LOC_Z=$Z_LOC" >> "$CONFPATH"
            fi

            # Ensure this file exists
            touch "${STARLING_TMP_DIR}/px4fmu_vehicle${idx}"
        done

    fi

    ## Write vehicle configurations and labels into kind config copy into tmp
    cp "${STARLING_CONFIG_DIR}/kind_config.yaml" "${STARLING_TMP_DIR}/kind_config.yaml"
    for (( idx=0; idx<TOTAL_NUM_VEH; idx++ )); do
echo "
- role: worker
  labels:
    starling.dev/type: vehicle
  extraMounts:
  - hostPath: ${STARLING_TMP_DIR}/vehicle${idx}.config
    containerPath: /etc/starling/vehicle.config
    readOnly: true
  - hostPath: ${STARLING_TMP_DIR}/px4fmu_vehicle${idx}
    containerPath: /dev/px4fmu
    readOnly: true
" >> "${STARLING_TMP_DIR}/kind_config.yaml"
    done

    # Start local registry
    # create registry container unless it already exists
    reg_name="${STARLING_REGISTRY_NAME}"
    reg_port="${STARLING_REGISTRY_DEFAULT_PORT}"
    if [ "$(docker inspect -f '{{.State.Running}}' "${reg_name}" 2>/dev/null || true)" != 'true' ]; then
        docker run \
            -d --restart=always -p "127.0.0.1:${reg_port}:5000" --name "${reg_name}" \
            registry:2
        echo "Local registry running on localhost:${reg_port}"
    fi

tee -a  "${STARLING_TMP_DIR}/kind_config.yaml" << END
containerdConfigPatches:
- |-
  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."localhost:${reg_port}"]
    endpoint = ["http://${reg_name}:5000"]
END

    ## Start cluster with new configuration
    kind create cluster --config="${STARLING_TMP_DIR}/kind_config.yaml"

    # connect the registry to the cluster network if not already connected
    if [ "$(docker inspect -f='{{json .NetworkSettings.Networks.kind}}' "${reg_name}")" = 'null' ]; then
    docker network connect "kind" "${reg_name}"
    fi

    # Document the local registry
    # https://github.com/kubernetes/enhancements/tree/master/keps/sig-cluster-lifecycle/generic/1755-communicating-a-local-registry
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: local-registry-hosting
  namespace: kube-public
data:
  localRegistryHosting.v1: |
    host: "localhost:${reg_port}"
    help: "https://kind.sigs.k8s.io/docs/user/local-registry/"
EOF

fi

# Start Dashboard
if [[ $S_DASH ]]; then
    echo "Deploying Kubernetes Dashboard"
    echo "See: https://rancher.com/docs/k3s/latest/en/installation/kube-dashboard/ for more details"
    echo "==================="
    GITHUB_URL=https://github.com/kubernetes/dashboard/releases
    VERSION_KUBE_DASHBOARD=$(curl -w '%{url_effective}' -I -L -s -S ${GITHUB_URL}/latest -o /dev/null | sed -e 's|.*/||')
    kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
    kubectl apply -f ${STARLING_WORKDIR}/kubernetes/resources/dashboard.admin.yaml #dashboard.admin-user.yml -f deployment/resources/dashboard.admin-user-role.yml
    echo "==================="
       
    echo "Creating Dashboard Token:"
    DASHBOARD_TOKEN="$(kubectl -n kubernetes-dashboard create token admin-user)"

    echo "The Dashboard is available at https://localhost:31771"
    echo "You will need the dashboard token, to access it."
    if command -v xclip; then
        set +e # Will fail if there is no xserver running, e.g. via ssh. Fail silently
        echo $DASHBOARD_TOKEN | xclip -selection clipboard -i
        echo "The token has been copied onto your clipboard, it is also printed below"
        set -e
    else
        echo "Copy and paste the token from below"
    fi
    echo "-----BEGIN DASHBOARD TOKEN-----"
    echo $DASHBOARD_TOKEN
    echo "-----END DASHBOARD TOKEN-----"
    echo "Note: your browser may not like the self signed ssl certificate, ignore and continue for now"
    echo "To get the token yourself run: kubectl -n kubernetes-dashboard create token admin-user"
    echo "==================="
fi
